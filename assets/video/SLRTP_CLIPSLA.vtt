WEBVTT

00:00:00.000 --> 00:00:03.000
Hello, my name is Sarah Alyami

00:00:03.000 --> 00:00:06.000
I will present our paper titled 

00:00:06.000 --> 00:00:09.000
CLIP-SLA: Parameter Efficient CLIP Adaptation for Continuous Sign Language Recognition

00:00:09.000 --> 00:00:12.000
This work is in collaboration with

00:00:12.000 --> 00:00:15.000
Dr. Hamza Luqman at King Fahd University of Petroleum and Minerals

00:00:16.000 --> 00:00:19.000
Starting with the motivation behind this

00:00:19.000 --> 00:00:22.000
work while Continuous Sign Language Recognition has seen significant progress

00:00:22.000 --> 00:00:27.000
current frameworks mostly rely on visual backbones pretrained on visual data.  

00:00:27.000 --> 00:00:30.500
However, recently vision-language models like CLIP

00:00:30.500 --> 00:00:34.000
have demonstrated impressive generalization across a wide range of tasks

00:00:34.000 --> 00:00:39.000
by learning strong semantic representations from large-scale imageâ€“text data

00:00:39.000 --> 00:00:42.500
But despite their success, these models

00:00:42.500 --> 00:00:46.000
haven't been explored in the context of Continuous Sign Language Recognition

00:00:46.000 --> 00:00:48.666
In this work, we aimed to

00:00:48.666 --> 00:00:51.333
investigate how can we adapt the

00:00:51.333 --> 00:00:54.000
powerful representations of CLIP to benefit Continuous Sign Language Recognition

00:00:55.000 --> 00:00:57.333
However, using CLIP directly in our

00:00:57.333 --> 00:00:59.666
task is challenging because first the

00:00:59.666 --> 00:01:02.000
CLIP model is designed for image understanding 

00:01:02.000 --> 00:01:04.000
and lacks temporal modeling, which is

00:01:04.000 --> 00:01:06.000
necessary for Continuous Sign Language Recognition

00:01:06.000 --> 00:01:12.000
Also, now naive full fine-tuning risks overwriting its pretrained representations

00:01:12.000 --> 00:01:16.000
To overcome these challenges, we proposed

00:01:16.000 --> 00:01:20.000
frameworks that integrate temporal modules within CLIP

00:01:20.000 --> 00:01:23.500
Also, instead of fully fine-tuning CLIP,

00:01:23.500 --> 00:01:27.000
we proposed using parameter-efficient fine-tuning techniques

00:01:27.000 --> 00:01:30.333
As a result, we proposed

00:01:30.333 --> 00:01:33.666
CLIP-SLA or CLIP Sign Language Adaptation,

00:01:33.666 --> 00:01:37.000
a parameter-efficient framework that adapts the CLIP visual encoder for 

00:01:37.000 --> 00:01:40.000
Continuous Sign Language Recognition without fully fine-tuning it

00:01:40.000 --> 00:01:43.000
The adapted CLIP visual encoder is

00:01:43.000 --> 00:01:46.000
integrated with the standard sequence Module in Continuous Sign Language Recognition

00:01:46.000 --> 00:01:49.500
We keep the CLIP weights frozen

00:01:49.500 --> 00:01:53.000
using the Vision Transformer backbone and introduce two variants

00:01:53.000 --> 00:02:00.000
SLA-Adapter and SLA-LoRA to inject task-specific understanding

00:02:00.000 --> 00:02:03.500
I'll explain each of these two variants

00:02:03.500 --> 00:02:07.000
briefly starting with SLA Lora

00:02:14.333 --> 00:02:18.000
SLA Lora uses low rank rank adaptation to modify the weights 

00:02:18.000 --> 00:02:21.000
within the attention and feed-forward blocks of the CLIP backbone

00:02:21.000 --> 00:02:25.333
We also incorporate Temporal Shift Module,

00:02:25.333 --> 00:02:29.666
which shifts a few channels across

00:02:29.666 --> 00:02:34.000
time steps, enabling cross-frame temporal understanding

00:02:35.000 --> 00:02:43.000
Moving on to the second variant SLA-Adapter,

00:02:43.000 --> 00:02:46.500
In SLA Adaptor, we experimented with

00:02:46.500 --> 00:02:50.000
a different method of adaptation by inserting lightweight trainable adapter blocks 

00:02:50.000 --> 00:02:52.666
The adapters include depth-wise 3D convolutions

00:02:52.666 --> 00:02:55.333
to efficiently learn spatial temporal futures

00:02:55.333 --> 00:02:58.000
while keeping the CLIP weights frozen

00:02:58.000 --> 00:03:03.500
Both variants SLA-Lora and SLA-Adapter

00:03:03.500 --> 00:03:09.000
avoid the cost of full fine-tuning and enable temporal reasoning

00:03:09.000 --> 00:03:12.000
We evaluated our frameworks on four

00:03:12.000 --> 00:03:15.000
datasets, including Phoenix2014 and

00:03:15.000 --> 00:03:18.000
also on a new dataset Isharah-500

00:03:18.000 --> 00:03:22.000
which is a diverse dataset

00:03:22.000 --> 00:03:26.000
in Saudi sign language recorded using smartphone cameras

00:03:26.000 --> 00:03:29.500
Here are the results reported in

00:03:29.500 --> 00:03:33.000
Word Error Rate, where lower Word Error Rate indicates better performance

00:03:33.000 --> 00:03:36.000
Our frameworks achieved competitive performance across

00:03:36.000 --> 00:03:39.000
all three datasets, ranking 

00:03:39.000 --> 00:03:42.000
second after the SlowFastSign framework

00:03:43.000 --> 00:03:46.000
We conducted an efficiency analysis to

00:03:46.000 --> 00:03:49.000
understand the benefits our proposed adaptation

00:03:49.000 --> 00:03:52.000
strategies, including the number of tunable

00:03:53.000 --> 00:03:56.000
and training time

00:03:56.000 --> 00:03:59.000
Here we show the number of tunable parameters versus the Word Error Rate

00:03:59.000 --> 00:04:04.000
Our frameworks achieved strong balance between accuracy and efficiency.

00:04:04.000 --> 00:04:07.500
For example, SLA-Adaptor has 41%

00:04:07.500 --> 00:04:11.000
fewer tunable parameters than the best

00:04:11.000 --> 00:04:14.500
models SlowFastSign, while achieving

00:04:14.500 --> 00:04:18.000
comparable performance with only 0.5 Word Error Rate difference

00:04:19.000 --> 00:04:21.333
As for the results on the

00:04:21.333 --> 00:04:23.666
Isharah-500 dataset, our frameworks

00:04:23.666 --> 00:04:26.000
outperformed previous baselines with a significant margin

00:04:26.000 --> 00:04:28.750
These results validate the robustness of

00:04:28.750 --> 00:04:31.500
our frameworks in handling realistic and

00:04:31.500 --> 00:04:34.250
challenging scenarios, such as poor lighting

00:04:34.250 --> 00:04:37.000
and clutter backgrounds 

00:04:38.000 --> 00:04:40.399
We ran extensive ablations such as

00:04:40.399 --> 00:04:42.800
component analysis that verified the effectiveness

00:04:42.800 --> 00:04:45.199
of the proposed modules, also conducted

00:04:45.199 --> 00:04:47.600
ablations to find the best settings

00:04:47.600 --> 00:04:50.000
for the LoRA and adapter blocks.

00:04:54.000 --> 00:04:56.199
We also visualized the class activation

00:04:56.199 --> 00:04:58.399
maps, which showed that our models

00:04:58.399 --> 00:05:00.600
can effectively focus on the informative

00:05:00.600 --> 00:05:02.800
regions, as you can see here,

00:05:02.800 --> 00:05:05.000
with focus on the hands and mouth regions

00:05:06.000 --> 00:05:09.333
In conclusion, the CLIP-SLA frameworks

00:05:09.333 --> 00:05:12.666
effectively adapt CLIP for 

00:05:12.666 --> 00:05:16.000
continuous Sign Language Recognition using parameter efficient fine-tuning

00:05:16.000 --> 00:05:18.600
The findings of this paper highlight

00:05:18.600 --> 00:05:21.199
the potential of pre-trained vision language

00:05:21.199 --> 00:05:23.800
models for Continuous Sign Language Recognition,

00:05:23.800 --> 00:05:26.399
and future work can explore other

00:05:26.399 --> 00:05:29.000
adaptation techniques such as visual prompt tuning

00:05:30.000 --> 00:05:32.000
Thank you everyone
